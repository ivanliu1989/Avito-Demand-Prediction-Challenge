library(data.table)
train_dat = fread('./data/train.csv')
train_dat
library(tidyverse)
install.packages('tidyverse')
train_dat <- read_csv('./data/train.csv')
library(tidyverse)
train_dat <- read_csv('./data/train.csv')
class(train_dat)
train_dat
setDT(train_dat)
train_dat
train_dat <- read_csv('./data/train.csv')
train_dat$region
train_dat <- fread('./data/train.csv', encoding = 'UTF-8')
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
install.packages('text2vec')
install.packages('tokenizers')
install.packages('stopwords')
0
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
install.packages('tidyverse')
install.packages('tidyverse')
install.packages('lubridate')
install.packages('magrittr')
install.packages('text2vec')
install.packages('tokenizers')
install.packages('stopwords')
install.packages('xgboost')
install.packages('Matrix')
install.packages("Matrix")
install.packages("Matrix")
install.packages("data.table")
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
install.packages('data.table', repo='http://nbcgib.uesc.br/mirrors/cran/')
install.packages('data.table', repo='http://nbcgib.uesc.br/mirrors/cran/')
install.packages('data.table')
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
#---------------------------
cat("Loading data...\n")
tr <- read_csv("../data/train.csv")
te <- read_csv("../data/test.csv")
tr <- read_csv("./data/train.csv")
te <- read_csv("./data/test.csv")
#---------------------------
cat("Preprocessing...\n")
tri <- 1:nrow(tr)
y <- tr$deal_probability
y
tr_te <- tr %>%
select(-deal_probability) %>%
bind_rows(te) %>%
mutate(category_name = as_factor(category_name),
parent_category_name = as_factor(parent_category_name),
region = as_factor(region),
user_type = as_factor(user_type),
price = log1p(price),
txt = paste(city, param_1, param_2, param_3, title, description, sep = " "),
mon = month(activation_date),
mday = mday(activation_date),
week = week(activation_date),
wday = wday(activation_date)) %>%
select(-item_id, -user_id, -city, -param_1, -param_2, -param_3,
-title, -description, -activation_date, -image) %>%
replace_na(list(image_top_1 = -1, price = -1)) %T>%
glimpse()
tr_te
rm(tr, te); gc()
#---------------------------
cat("Parsing text...\n")
it <- tr_te %$%
str_to_lower(txt) %>%
str_replace_all("[^[:alpha:]]", " ") %>%
str_replace_all("\\s+", " ") %>%
tokenize_word_stems(language = "russian") %>%
itoken()
it
vect <- create_vocabulary(it, ngram = c(1, 1), stopwords = stopwords("ru")) %>%
prune_vocabulary(term_count_min = 3, doc_proportion_max = 0.3, vocab_term_max = 4000) %>%
vocab_vectorizer()
m_tfidf <- TfIdf$new(norm = "l2", sublinear_tf = T)
tfidf <-  create_dtm(it, vect) %>%
fit_transform(m_tfidf)
rm(it, vect, m_tfidf); gc()
#---------------------------
cat("Preparing data...\n")
X <- tr_te %>%
select(-txt) %>%
sparse.model.matrix(~ . - 1, .) %>%
cbind(tfidf)
rm(tr_te, tfidf); gc()
dtest <- xgb.DMatrix(data = X[-tri, ])
X <- X[tri, ]; gc()
tri <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
dtrain <- xgb.DMatrix(data = X[tri, ], label = y[tri])
dval <- xgb.DMatrix(data = X[-tri, ], label = y[-tri])
cols <- colnames(X)
rm(X, y, tri); gc()
#---------------------------
cat("Training model...\n")
p <- list(objective = "reg:logistic",
booster = "gbtree",
eval_metric = "rmse",
nthread = 8,
eta = 0.05,
max_depth = 7,
min_child_weight = 1,
gamma = 0,
subsample = 0.7,
colsample_bytree = 0.7,
alpha = 0,
lambda = 0,
nrounds = 1000)
m_xgb <- xgb.train(p, dtrain, p$nrounds, list(val = dval), print_every_n = 50, early_stopping_rounds = 50)
install.packages('caret')
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
set.seed(0)
#---------------------------
cat("Loading data...\n")
tr <- read_csv("./data/train.csv")
te <- read_csv("./data/test.csv")
#---------------------------
cat("Preprocessing...\n")
tri <- 1:nrow(tr)
y <- tr$deal_probability
tr_te <- tr %>%
select(-deal_probability) %>%
bind_rows(te) %>%
mutate(category_name = as_factor(category_name),
parent_category_name = as_factor(parent_category_name),
region = as_factor(region),
user_type = as_factor(user_type),
price = log1p(price),
txt = paste(city, param_1, param_2, param_3, title, description, sep = " "),
mon = month(activation_date),
mday = mday(activation_date),
week = week(activation_date),
wday = wday(activation_date)) %>%
select(-item_id, -user_id, -city, -param_1, -param_2, -param_3,
-title, -description, -activation_date, -image) %>%
replace_na(list(image_top_1 = -1, price = -1)) %T>%
glimpse()
rm(tr, te); gc()
#---------------------------
cat("Parsing text...\n")
it <- tr_te %$%
str_to_lower(txt) %>%
str_replace_all("[^[:alpha:]]", " ") %>%
str_replace_all("\\s+", " ") %>%
tokenize_word_stems(language = "russian") %>%
itoken()
vect <- create_vocabulary(it, ngram = c(1, 1), stopwords = stopwords("ru")) %>%
prune_vocabulary(term_count_min = 3, doc_proportion_max = 0.3, vocab_term_max = 4000) %>%
vocab_vectorizer()
m_tfidf <- TfIdf$new(norm = "l2", sublinear_tf = T)
tfidf <-  create_dtm(it, vect) %>%
fit_transform(m_tfidf)
rm(it, vect, m_tfidf); gc()
#---------------------------
cat("Preparing data...\n")
X <- tr_te %>%
select(-txt) %>%
sparse.model.matrix(~ . - 1, .) %>%
cbind(tfidf)
rm(tr_te, tfidf); gc()
dtest <- xgb.DMatrix(data = X[-tri, ])
X <- X[tri, ]; gc()
tri <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
dtrain <- xgb.DMatrix(data = X[tri, ], label = y[tri])
dval <- xgb.DMatrix(data = X[-tri, ], label = y[-tri])
cols <- colnames(X)
rm(X, y, tri); gc()
#---------------------------
cat("Training model...\n")
p <- list(objective = "reg:logistic",
booster = "gbtree",
eval_metric = "rmse",
nthread = 8,
eta = 0.05,
max_depth = 7,
min_child_weight = 1,
gamma = 0,
subsample = 0.7,
colsample_bytree = 0.7,
alpha = 0,
lambda = 0,
nrounds = 1000)
m_xgb <- xgb.train(p, dtrain, p$nrounds, list(val = dval), print_every_n = 50, early_stopping_rounds = 50)
library(tidyverse)
tr <- read_csv("./data/train.csv")
library(data.table)
library(readr)
dat = read_csv('../Avito-Demand-Prediction-Challenge/data/train.csv')
setDT(dat)
dat
Sys.setlocale(,"ru_RU")
Sys.setlocale(,"russian")
library(data.table)
library(readr)
dat = read_csv('../Avito-Demand-Prediction-Challenge/data/train.csv')
setDT(dat)
dat
dat$title
head(dat$title)
head(tr$title)
head(dat$title)
identical(dat$title, tr$title)
tri <- 1:nrow(tr)
y <- tr$deal_probability
rm(list=ls());gc()
tr = read_csv('../Avito-Demand-Prediction-Challenge/data/train.csv')
setDT(tr)
te = read_csv('../Avito-Demand-Prediction-Challenge/data/train.csv')
setDT(te)
te
tr[, train_flag := 1]
te[, train_flag := 0]
y <- tr$deal_probability
dat = rbind(tr, te)
dat
str(dat)
# Feature engineering -----------------------------------------------------
dat[,price := log(price)]
dat
dat[,txt := paste(region, city, param_1, param_2, param_3, title, description, sep = " ")]
dat[, mon := month(activation_date)]
dat[, mday := mday(activation_date)]
dat[, week := week(activation_date)]
dat[, wday := wday(activation_date)]
dat
dat[, price := ifelse(is.na(price), -1, price)]
dat[, image_top_1 := ifelse(is.na(image_top_1), -1, image_top_1)]
gc()
dat
?str_replace_all
dat[, txt := str_to_lower(txt)]
dat[, txt := str_replace_all(txt, "[^[:alpha:]]", " ")]
dat[, txt := str_replace_all(txt, "\\s+", " ")]
?tokenize_word_stems
library(tidyverse)
library(lubridate)
library(magrittr)
library(text2vec)
library(tokenizers)
library(stopwords)
library(xgboost)
library(Matrix)
?tokenize_word_stems
tokenized = tokenize_word_stems(txt, language = "russian", stopwords = NULL)
tokenized = tokenize_word_stems(dat$txt, language = "russian", stopwords = NULL)
tokenized
token = itoken(tokenized)
token
?itoken
vect = create_vocabulary(token, ngram = c(1, 1), stopwords = stopwords("ru"))
vect
vect = create_vocabulary(token, ngram = c(1, 1), stopwords = stopwords("ru"))
vect
vect = prune_vocabulary(vect, term_count_min = 3, doc_proportion_max = 0.3, vocab_term_max = 4000)
vect
vect = vocab_vectorizer(vect)
vect
?TfIdf
m_tfidf <- TfIdf$new(norm = "l2", sublinear_tf = T)
tfidf <-  create_dtm(token, vect)
tfidf
tfidf <-  create_dtm(token, vect) %>%
fit_transform(m_tfidf)
?sparse.model.matrix
dat.sparse = sparse.model.matrix(dat)
dat.sparse = sparse.model.matrix(~., dat)
dim(dat.sparse)
gc()
dat.sparse
dim(dat)
dim(tfidf)
dim(dat.sparse)
dat_all = cbind(dat, tfidf)
dim(dat_all)
dim(tfidf)
dat_all = cbind(dat, tfidf)
dat_all
dat
?sparse.model.matrix
class(tfidf)
?sparseMatrix
?sparse.model.matrix
tr_idx = 1:nrow(dat[train_flag==1])
tr_idx
dat[train_flag==1]
tfidf.dt = as.data.table(tfidf)
tfidf.dt = as.data.table(as.matrix(tfidf))
dat[, -col_to_drop]
col_to_drop = c('item_id', 'user_id', 'city', 'param_1', 'param_2', 'param_3', 'title', 'description', 'activation_date', 'image')
dat[, -col_to_drop]
dat[, -col_to_drop, with = F]
dat[, !col_to_drop, with = F]
col_to_drop = c('item_id', 'user_id', 'city', 'param_1', 'param_2', 'param_3', 'title', 'description', 'activation_date', 'image', 'txt')
dat[, !col_to_drop, with = F]
dat.sparse = sparse.model.matrix(~ -1 + ., dat[, !col_to_drop, with = F], -1)
dat.sparse
dat_all = cbind(dat.sparse, tfidf)
dat_all
train = dat_all[tr_idx,]
test = dat_all[-tr_idx,]
train
gc()
# Modeling ----------------------------------------------------------------
dtest <- xgb.DMatrix(data = test)
tri <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
tri
dtrain <- xgb.DMatrix(data = train[tri, ], label = y[tri])
dval <- xgb.DMatrix(data = train[-tri, ], label = y[-tri])
cols <- colnames(X)
gc()
cols <- colnames(train)
cols
m_xgb <- xgb.train(p, dtrain, p$nrounds, list(val = dval), print_every_n = 50, early_stopping_rounds = 50)
cat("Training model...\n")
p <- list(objective = "reg:logistic",
booster = "gbtree",
eval_metric = "rmse",
nthread = 8,
eta = 0.05,
max_depth = 13,
min_child_weight = 2,
gamma = 0,
subsample = 0.7,
colsample_bytree = 0.7,
alpha = 0,
lambda = 0,
nrounds = 1500)
m_xgb <- xgb.train(p, dtrain, p$nrounds, list(val = dval), print_every_n = 50, early_stopping_rounds = 50)
dat$deal_probability
col_to_drop = c('item_id', 'user_id', 'city', 'param_1', 'param_2', 'param_3', 'title', 'description', 'activation_date', 'image', 'txt', 'deal_probability')
dat.sparse = sparse.model.matrix(~ -1 + ., dat[, !col_to_drop, with = F], -1)
dat_all = cbind(dat.sparse, tfidf)
train = dat_all[tr_idx,]
test = dat_all[-tr_idx,]
gc()
# Modeling ----------------------------------------------------------------
dtest <- xgb.DMatrix(data = test)
tri <- caret::createDataPartition(y, p = 0.9, list = F) %>% c()
dtrain <- xgb.DMatrix(data = train[tri, ], label = y[tri])
dval <- xgb.DMatrix(data = train[-tri, ], label = y[-tri])
cols <- colnames(train)
gc()
#---------------------------
cat("Training model...\n")
p <- list(objective = "reg:logistic",
booster = "gbtree",
eval_metric = "rmse",
nthread = 8,
eta = 0.05,
max_depth = 13,
min_child_weight = 2,
gamma = 0,
subsample = 0.7,
colsample_bytree = 0.7,
alpha = 0,
lambda = 0,
nrounds = 1500)
m_xgb <- xgb.train(p, dtrain, p$nrounds, list(val = dval), print_every_n = 50, early_stopping_rounds = 50)
xgb.importance(cols, model=m_xgb) %>%
xgb.plot.importance(top_n = 15)
#---------------------------
cat("Creating submission file...\n")
read_csv("./submissions/sample_submission.csv") %>%
mutate(deal_probability = predict(m_xgb, dtest)) %>%
write_csv(paste0("xgb_tfidf", round(m_xgb$best_score, 5), ".csv"))
read_csv("./data/sample_submission.csv") %>%
mutate(deal_probability = predict(m_xgb, dtest)) %>%
write_csv(paste0("./submissions/xgb_tfidf", round(m_xgb$best_score, 5), ".csv"))
dim(test)
dim(dat_all)
dim(dat_all)
dim(train)
dim(dat)
dim(tr)
