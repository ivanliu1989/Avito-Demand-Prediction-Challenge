{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "use_cols = ['param_1','param_2','param_3','title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_text(start):\n",
    "    print('Loading data...', end='')\n",
    "    tic = time.time()\n",
    "    train2 = pd.read_csv('../input/train_active.csv', usecols=use_cols, nrows= 1000000, skiprows=range(1, start))\n",
    "    toc = time.time()\n",
    "    print('Done in {:.1f}s'.format(toc-tic))\n",
    "    train2['text'] = train2['param_1'].str.cat([train2.param_2,train2.param_3,train2.title,train2.description], sep=' ',na_rep='')\n",
    "    train2.drop(use_cols, axis = 1, inplace=True)\n",
    "    train2 = train2['text'].values\n",
    "\n",
    "    train2 = [text_to_word_sequence(text) for text in tqdm(train2)]\n",
    "    return train2\n",
    "\n",
    "model = Word2Vec(size=100, window=5,max_vocab_size=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(15):\n",
    "    update = False\n",
    "    if k != 0:\n",
    "        update = True\n",
    "    train = load_text(k*1000000+1)\n",
    "    model.build_vocab(train, update=update)\n",
    "    model.train(train, total_examples=model.corpus_count, epochs=3)\n",
    "\n",
    "model.save('avito.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using self-trained embeddings from train_active on the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2fff32e4479a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "EMBEDDING = '../input/using-train-active-for-training-word-embeddings/avito.w2v'\n",
    "TRAIN_CSV = '../input/avito-demand-prediction/train.csv'\n",
    "TEST_CSV = '../input/avito-demand-prediction/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 100\n",
    "embed_size = 100\n",
    "train = pd.read_csv(TRAIN_CSV, index_col = 0)\n",
    "labels = train[['deal_probability']].copy()\n",
    "train = train[['description']].copy()\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "\n",
    "print('fitting tokenizer...',end='')\n",
    "train['description'] = train['description'].astype(str)\n",
    "tokenizer.fit_on_texts(list(train['description'].fillna('NA').values))\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train['description'].values, labels['deal_probability'].values, test_size = 0.1, random_state = 23)\n",
    "\n",
    "print('convert to sequences...',end='')\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "print('done.')\n",
    "print('padding...',end='')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=maxlen)\n",
    "print('done.')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape = (maxlen, ))\n",
    "    emb = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n",
    "                    input_length = maxlen, trainable = False)(inp)\n",
    "    main = SpatialDropout1D(0.2)(emb)\n",
    "    main = Bidirectional(CuDNNGRU(32,return_sequences = True))(main)\n",
    "    main = GlobalAveragePooling1D()(main)\n",
    "    main = Dropout(0.2)(main)\n",
    "    out = Dense(1, activation = \"sigmoid\")(main)\n",
    "\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=0.001), loss = 'mean_squared_error',\n",
    "                  metrics =[root_mean_squared_error])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets train our model for four epochs and save the best epoch.\n",
    "EPOCHS = 4\n",
    "file_path = \"model.hdf5\"\n",
    "\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 256, epochs = EPOCHS, validation_data = (X_valid, y_valid),\n",
    "                verbose = 1, callbacks = [check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "prediction = model.predict(X_valid)\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats some improvement compared to using the pre-trained embedding model which scored 0.2370. Additionally since the embeddings here are trained also on param_1, param_2, param_3 and title which have much more out of vocabulary words when using Fasttext. Hence self-trained embeddings are clearly performing better.\n",
    "Ok, now we are ready to do a submission and compare the LB score with the pre-trained embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_CSV, index_col = 0)\n",
    "test = test[['description']].copy()\n",
    "\n",
    "test['description'] = test['description'].astype(str)\n",
    "X_test = test['description'].values\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print('padding')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_test,batch_size = 128, verbose = 1)\n",
    "\n",
    "sample_submission = pd.read_csv('../input/avito-demand-prediction/sample_submission.csv', index_col = 0)\n",
    "submission = sample_submission.copy()\n",
    "submission['deal_probability'] = prediction\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
