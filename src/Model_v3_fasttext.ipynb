{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (tensorflow.py, line 26)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2862\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-3-7467bc9b2288>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from keras.preprocessing import text, sequence\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\__init__.py\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from . import utils\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\__init__.py\"\u001b[0m, line \u001b[0;32m6\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from . import conv_utils\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\conv_utils.py\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from .. import backend as K\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\__init__.py\"\u001b[0m, line \u001b[0;32m83\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from .tensorflow_backend import *\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import tensorflow as tf\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\Projects\\Avito-Demand-Prediction-Challenge\\src\\tensorflow.py\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    print(f'[{name}] done in {time.time() - t0:.0f} s')\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/fasttest-common-crawl-russian/cc.ru.300.vec'\n",
    "TRAIN_CSV = '../input/avito-demand-prediction/train.csv'\n",
    "TEST_CSV = '../input/avito-demand-prediction/test.csv'\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 100\n",
    "embed_size = 300\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV, index_col = 0)\n",
    "labels = train[['deal_probability']].copy()\n",
    "train = train[['description']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "\n",
    "train['description'] = train['description'].astype(str)\n",
    "tokenizer.fit_on_texts(list(train['description'].fillna('NA').values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('getting embeddings')\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open(EMBEDDING_FILE)))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del embeddings_index\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train['description'].values, labels['deal_probability'].values, test_size = 0.1, random_state = 23)\n",
    "del train\n",
    "print('convert to sequences')\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('padding')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape = (maxlen, ))\n",
    "    emb = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n",
    "                    input_length = maxlen, trainable = False)(inp)\n",
    "    main = SpatialDropout1D(0.2)(emb)\n",
    "    main = Bidirectional(CuDNNGRU(32,return_sequences = True))(main)\n",
    "    main = GlobalAveragePooling1D()(main)\n",
    "    main = Dropout(0.2)(main)\n",
    "    out = Dense(1, activation = \"sigmoid\")(main)\n",
    "\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=0.001), loss = 'mean_squared_error',\n",
    "                  metrics =[root_mean_squared_error])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "model = build_model()\n",
    "file_path = \"model.hdf5\"\n",
    "\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 256, epochs = EPOCHS, validation_data = (X_valid, y_valid),\n",
    "                verbose = 1, callbacks = [check_point])\n",
    "model.load_weights(file_path)\n",
    "prediction = model.predict(X_valid)\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_CSV, index_col = 0)\n",
    "test = test[['description']].copy()\n",
    "\n",
    "test['description'] = test['description'].astype(str)\n",
    "X_test = test['description'].values\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print('padding')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_test,batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../input/avito-demand-prediction/sample_submission.csv', index_col = 0)\n",
    "submission = sample_submission.copy()\n",
    "submission['deal_probability'] = prediction\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
