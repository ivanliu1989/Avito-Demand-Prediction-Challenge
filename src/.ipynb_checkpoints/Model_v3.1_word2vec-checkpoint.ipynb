{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (tensorflow.py, line 26)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2862\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-3-f914efb531d8>\"\u001b[0m, line \u001b[0;32m6\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from keras.preprocessing.text import text_to_word_sequence\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\__init__.py\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from . import utils\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\__init__.py\"\u001b[0m, line \u001b[0;32m6\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from . import conv_utils\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\conv_utils.py\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from .. import backend as K\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\__init__.py\"\u001b[0m, line \u001b[0;32m83\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    from .tensorflow_backend import *\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import tensorflow as tf\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\Projects\\Avito-Demand-Prediction-Challenge\\src\\tensorflow.py\"\u001b[1;36m, line \u001b[1;32m26\u001b[0m\n\u001b[1;33m    print(f'[{name}] done in {time.time() - t0:.0f} s')\u001b[0m\n\u001b[1;37m                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from random import shuffle\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "use_cols = ['param_1','param_2','param_3','title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/c8/e2e6cb141aea53927aa1d554ab5919e202e61c2292df07c0c28d833dcf90/gensim-3.4.0-cp35-cp35m-win_amd64.whl (22.5MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (1.14.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from gensim) (0.19.1)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/69/c92661a333f733510628f28b8282698b62cdead37291c8491f3271677c02/smart_open-1.5.7.tar.gz\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/b7/a88a67002b1185ed9a8e8a6ef15266728c2361fcb4f1d02ea331e4c7741d/boto-2.48.0-py2.py3-none-any.whl (1.4MB)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.18.1)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/1c/898ab9025a1725d15c3b121f6c91642a2535acc5d363acb328d6b37ff6d1/boto3-1.7.40-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.6,>=2.5 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2.5)\n",
      "Requirement already satisfied: urllib3<1.22,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (1.21.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2017.4.17)\n",
      "Collecting botocore<1.11.0,>=1.10.40 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/6f/e9c3981f8b7e93bfa4461b754563b0e917968947920d0bdcf2a7dcf77da2/botocore-1.10.40-py2.py3-none-any.whl (4.3MB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from botocore<1.11.0,>=1.10.40->boto3->smart-open>=1.2.1->gensim) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from botocore<1.11.0,>=1.10.40->boto3->smart-open>=1.2.1->gensim) (2.6.0)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open: started\n",
      "  Running setup.py bdist_wheel for smart-open: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ivan Liu\\AppData\\Local\\pip\\Cache\\wheels\\b1\\9e\\7d\\bb3d3b55c597e72617140a0638c06382a5f17283881eae163e\n",
      "  Running setup.py bdist_wheel for bz2file: started\n",
      "  Running setup.py bdist_wheel for bz2file: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Ivan Liu\\AppData\\Local\\pip\\Cache\\wheels\\81\\75\\d6\\e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: boto, bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.48.0 boto3-1.7.40 botocore-1.10.40 bz2file-0.98 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.7\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_text(start):\n",
    "    print('Loading data...', end='')\n",
    "    tic = time.time()\n",
    "    train2 = pd.read_csv('../input/train_active.csv', usecols=use_cols, nrows= 1000000, skiprows=range(1, start))\n",
    "    toc = time.time()\n",
    "    print('Done in {:.1f}s'.format(toc-tic))\n",
    "    train2['text'] = train2['param_1'].str.cat([train2.param_2,train2.param_3,train2.title,train2.description], sep=' ',na_rep='')\n",
    "    train2.drop(use_cols, axis = 1, inplace=True)\n",
    "    train2 = train2['text'].values\n",
    "\n",
    "    train2 = [text_to_word_sequence(text) for text in tqdm(train2)]\n",
    "    return train2\n",
    "\n",
    "model = Word2Vec(size=100, window=5,max_vocab_size=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(15):\n",
    "    update = False\n",
    "    if k != 0:\n",
    "        update = True\n",
    "    train = load_text(k*1000000+1)\n",
    "    model.build_vocab(train, update=update)\n",
    "    model.train(train, total_examples=model.corpus_count, epochs=3)\n",
    "\n",
    "model.save('avito.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using self-trained embeddings from train_active on the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2fff32e4479a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing import text, sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Input, SpatialDropout1D,Dropout, GlobalAveragePooling1D, CuDNNGRU, Bidirectional, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "EMBEDDING = '../input/using-train-active-for-training-word-embeddings/avito.w2v'\n",
    "TRAIN_CSV = '../input/avito-demand-prediction/train.csv'\n",
    "TEST_CSV = '../input/avito-demand-prediction/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 100\n",
    "embed_size = 100\n",
    "train = pd.read_csv(TRAIN_CSV, index_col = 0)\n",
    "labels = train[['deal_probability']].copy()\n",
    "train = train[['description']].copy()\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "\n",
    "print('fitting tokenizer...',end='')\n",
    "train['description'] = train['description'].astype(str)\n",
    "tokenizer.fit_on_texts(list(train['description'].fillna('NA').values))\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec.load(EMBEDDING)\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "    except KeyError:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train['description'].values, labels['deal_probability'].values, test_size = 0.1, random_state = 23)\n",
    "\n",
    "print('convert to sequences...',end='')\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "print('done.')\n",
    "print('padding...',end='')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_valid = sequence.pad_sequences(X_valid, maxlen=maxlen)\n",
    "print('done.')\n",
    "\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape = (maxlen, ))\n",
    "    emb = Embedding(nb_words, embed_size, weights = [embedding_matrix],\n",
    "                    input_length = maxlen, trainable = False)(inp)\n",
    "    main = SpatialDropout1D(0.2)(emb)\n",
    "    main = Bidirectional(CuDNNGRU(32,return_sequences = True))(main)\n",
    "    main = GlobalAveragePooling1D()(main)\n",
    "    main = Dropout(0.2)(main)\n",
    "    out = Dense(1, activation = \"sigmoid\")(main)\n",
    "\n",
    "    model = Model(inputs = inp, outputs = out)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr=0.001), loss = 'mean_squared_error',\n",
    "                  metrics =[root_mean_squared_error])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets train our model for four epochs and save the best epoch.\n",
    "EPOCHS = 4\n",
    "file_path = \"model.hdf5\"\n",
    "\n",
    "check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", mode = \"min\", save_best_only = True, verbose = 1)\n",
    "history = model.fit(X_train, y_train, batch_size = 256, epochs = EPOCHS, validation_data = (X_valid, y_valid),\n",
    "                verbose = 1, callbacks = [check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "prediction = model.predict(X_valid)\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats some improvement compared to using the pre-trained embedding model which scored 0.2370. Additionally since the embeddings here are trained also on param_1, param_2, param_3 and title which have much more out of vocabulary words when using Fasttext. Hence self-trained embeddings are clearly performing better.\n",
    "Ok, now we are ready to do a submission and compare the LB score with the pre-trained embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_CSV, index_col = 0)\n",
    "test = test[['description']].copy()\n",
    "\n",
    "test['description'] = test['description'].astype(str)\n",
    "X_test = test['description'].values\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print('padding')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "prediction = model.predict(X_test,batch_size = 128, verbose = 1)\n",
    "\n",
    "sample_submission = pd.read_csv('../input/avito-demand-prediction/sample_submission.csv', index_col = 0)\n",
    "submission = sample_submission.copy()\n",
    "submission['deal_probability'] = prediction\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
