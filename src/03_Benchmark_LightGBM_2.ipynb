{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "import gc; gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {} #Get Optimal\n",
    "train = pd.read_csv('../data/train.csv', dtype=dtypes)\n",
    "test = pd.read_csv('../data/test.csv', dtype=dtypes)\n",
    "# zips = ['train_jpg.zip','test_jpg.zip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Spending Pattern by using active data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a = pd.read_csv('../data/train_active.csv', usecols=['user_id','price'], dtype=dtypes)\n",
    "train_a = train_a.groupby('user_id')['price'].agg(['mean','count']); gc.collect()\n",
    "test_a = pd.read_csv('../data/test.csv', usecols=['user_id','price'], dtype=dtypes)\n",
    "test_a = test_a.groupby('user_id')['price'].agg(['mean','count']); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a = train_a.reset_index()\n",
    "train = pd.merge(train, train_a, how='left', on='user_id')\n",
    "del train_a; gc.collect()\n",
    "test_a = test_a.reset_index()\n",
    "test = pd.merge(test, test_a, how='left', on='user_id')\n",
    "del test_a; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user = train.groupby('user_id')['deal_probability'].agg(['mean']); gc.collect()\n",
    "train_user = train_user.reset_index()\n",
    "train = pd.merge(train, train_user, how='left', on='user_id')\n",
    "test = pd.merge(test, train_user, how='left', on='user_id')\n",
    "del train_user; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['item_id','user_id'], inplace=True)\n",
    "train['image'] = train['image'].map(lambda x: 1 if len(str(x))>0 else 0)\n",
    "test['image'] = test['image'].map(lambda x: 1 if len(str(x))>0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.drop(columns=['user_id'], inplace=True); gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lazy for now\n",
    "train['activation_date'] = pd.to_datetime(train['activation_date']).dt.day\n",
    "test['activation_date'] = pd.to_datetime(test['activation_date']).dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region\n",
      "city\n",
      "parent_category_name\n",
      "category_name\n",
      "param_1\n",
      "param_2\n",
      "param_3\n",
      "title\n",
      "description\n",
      "user_type\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['region', 'city', 'parent_category_name', 'category_name', 'param_1',\n",
    "       'param_2', 'param_3', 'title', 'description',  'user_type']\n",
    "for c in cat_cols:\n",
    "    if c in ['title','description']:\n",
    "        train[c + '_len'] = train[c].map(lambda x: len(str(x))) #Lenth\n",
    "        train[c + '_wc'] = train[c].map(lambda x: len(str(x).split(' '))) #Word Count\n",
    "\n",
    "        test[c + '_len'] = test[c].map(lambda x: len(str(x))) #Lenth\n",
    "        test[c + '_wc'] = test[c].map(lambda x: len(str(x).split(' '))) #Word Count\n",
    "        if c != 'description':\n",
    "            feature_cnt = 20\n",
    "            tfidf = feature_extraction.text.TfidfVectorizer(max_features=feature_cnt)\n",
    "            train_tfidf = pd.DataFrame(tfidf.fit_transform(train[c].astype(str)).toarray())\n",
    "            test_tfidf = pd.DataFrame(tfidf.transform(test[c].astype(str)).toarray())\n",
    "            cols = train_tfidf.columns\n",
    "            for i in range(feature_cnt):\n",
    "                train[c + '_tfidf_' + str(i)] = train_tfidf[cols[i]]\n",
    "                test[c + '_tfidf_' + str(i)] = test_tfidf[cols[i]]\n",
    "    if c != 'description':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train[c].unique())+list(test[c].unique()))\n",
    "        train[c] = lbl.transform(train[c].astype(str))\n",
    "        test[c] = lbl.transform(test[c].astype(str))\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(-1, inplace=True)\n",
    "test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's auc: 0.766321\tvalid_0's rmse: 0.229029\n",
      "[400]\tvalid_0's auc: 0.769675\tvalid_0's rmse: 0.226851\n",
      "[600]\tvalid_0's auc: 0.771806\tvalid_0's rmse: 0.225755\n",
      "[800]\tvalid_0's auc: 0.772713\tvalid_0's rmse: 0.22509\n",
      "[1000]\tvalid_0's auc: 0.773153\tvalid_0's rmse: 0.224642\n",
      "[1200]\tvalid_0's auc: 0.77366\tvalid_0's rmse: 0.224279\n",
      "[1400]\tvalid_0's auc: 0.77391\tvalid_0's rmse: 0.22402\n",
      "Early stopping, best iteration is:\n",
      "[1433]\tvalid_0's auc: 0.773952\tvalid_0's rmse: 0.223985\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "ex_col = ['item_id','user_id','deal_probability','description','mean_y']\n",
    "col = [c for c in train.columns if c not in ex_col]\n",
    "\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col], train.deal_probability.values, test_size=0.20, random_state=19)\n",
    "#https://www.kaggle.com/shujian/avito-lightgbm-starter\n",
    "params = {'learning_rate': 0.05, 'max_depth': 7, 'boosting': 'gbdt', 'objective': 'regression', 'metric': ['auc','rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 128, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\n",
    "model2 = lgb.train(params, lgb.Dataset(x1, label=y1), 3000, lgb.Dataset(x2, label=y2), verbose_eval=200, early_stopping_rounds=100)\n",
    "test['deal_probability'] = model2.predict(test[col], num_iteration=model2.best_iteration)\n",
    "test['deal_probability'] = test['deal_probability'].clip(0., 1.)\n",
    "test[['item_id','deal_probability']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.231 LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
